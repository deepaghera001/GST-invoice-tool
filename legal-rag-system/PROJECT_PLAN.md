# Legal RAG System - Project Plan

## ğŸ¯ OVERALL GOAL

> **Build a system that converts official Indian law PDFs into testable, traceable, confidence-scored calculators & documents, without AI making legal decisions.**

**Target Reliability:** 97-99% (measured by tests and confidence, not claims)

---

## ğŸ§­ CORE PRINCIPLES (NON-NEGOTIABLE)

1. âœ… **PDFs are the only truth** - never modified, always hashed
2. âœ… **AI never approves correctness** - only retrieves and proposes
3. âœ… **Rules are frozen before production** - immutable after freeze
4. âœ… **Production never uses AI** - only frozen deterministic rules
5. âœ… **Everything must be testable** - clear pass/fail criteria
6. âœ… **If unsure â†’ do nothing (fail safe)** - no guessing

**If any step violates these â†’ STOP**

---

## ğŸ”§ TECHNOLOGY STACK (LOCKED)

### Stage 0-1: Document Processing
- **PDF Parser:** `pdfjs-dist` (Mozilla PDF.js legacy build)
- **Runtime:** Node.js 20+
- **Storage:** Filesystem (JSON)

### Stage 1: Retrieval Layer
- **Vector DB:** ChromaDB (local, persistent)
- **Embeddings:** OpenAI `text-embedding-3-small` (1536 dimensions)
  - Fallback: None initially (fail-safe: return error, don't guess)
- **Orchestration:** Custom (Stage 1)
  - **LlamaIndex:** NOT USED initially
  - **When needed:** If Stage 2 requires multi-document reasoning (cross-referencing Finance Act + Income-tax Act)
  - **Decision gate:** After Stage 1.9 passes, before Stage 2.1

### Stage 2-3: Rule Extraction
- **LLM:** OpenAI GPT-4 (or Claude Sonnet)
  - ONLY for candidate generation, NEVER for approval
- **Prompt Management:** Hardcoded prompts (version controlled)
- **Output Format:** Structured JSON (schema-validated)

### Stage 4: Schema Enforcement
- **Validation:** Zod or JSON Schema
- **DSL Storage:** TypeScript interfaces + runtime validators

### Stage 5-7: Testing & Production
- **Test Framework:** Vitest or Jest
- **Frozen Rules Storage:** Immutable JSON files (git-tracked)
- **Production Runtime:** Pure TypeScript (NO AI calls)

### Cross-Cutting
- **Graph DB:** Optional (Stage 3+)
  - **Tool:** Neo4j or embedded graph library
  - **Purpose:** Rule dependencies, cross-act references, proviso chains
  - **Trigger:** ONLY if >20% of rules have cross-references
  - **NOT for production execution** - only for candidate validation
  - **Decision:** After Stage 2 extracts first 50 rules
- **Audit Logging:** Filesystem (append-only JSON logs)

---

## ğŸ§± PIPELINE OVERVIEW (8 STAGES)

```
Stage 0: Document Authority (PDFs + Hashing)
    â†“
Stage 1: Retrieval (RAG - Text Extraction & Embedding)
    â†“
Stage 2: Rule Candidates (AI Proposes Rules)
    â†“
Stage 3: Alignment Validation (Check Alignment, Not Correctness)
    â†“
Stage 4: Schema Mapping (Force Rules into Fixed Structure)
    â†“
Stage 5: Deterministic Testing (Boundary, Regression, Parity Tests)
    â†“
Stage 6: Freeze Gate (Governance - Approve for Production)
    â†“
Stage 7: Production Execution (Frozen Rules Only, No AI)
```

---

## ğŸ“Š PROGRESS TRACKING

### Stage 0: Document Authority âœ… COMPLETE
**Goal:** Store PDFs with proof of version used

| Task | Status | Success Criteria |
|------|--------|------------------|
| Create folder structure | âœ… | All folders exist |
| Download Finance Act 2024 | âœ… | PDF in correct location |
| Generate SHA-256 hash | âœ… | Hash file created |
| Create metadata | âœ… | metadata.json exists |
| Register in documents.json | âœ… | Document registered |

**KPI:** Hash stable âœ…

---

### Stage 1: Retrieval (RAG) âš™ï¸ 60% COMPLETE
**Goal:** Extract and embed text for semantic search (AI retrieves, doesn't interpret)

| Task | Type | Status | Success Criteria |
|------|------|--------|------------------|
| 1.1 PDF parsing library | Infrastructure | âœ… | pdfjs-dist installed and working |
| 1.2 Parse PDF to structured text | Infrastructure | âœ… | 26/26 pages extracted with page numbers |
| 1.3 Semantic-preserving chunking | Infrastructure | âœ… | 44 chunks, no mid-sentence breaks, page refs |
| 1.4 Install ChromaDB | Vector DB | â¬œ | `chromadb` npm package installed |
| 1.5 Generate embeddings | Vector DB | â¬œ | All 44 chunks embedded (OpenAI API) |
| 1.6 Store in ChromaDB | Vector DB | â¬œ | Collection created, chunks inserted |
| 1.7 Test retrieval - known facts | Validation | â¬œ | Query "section 115BAC" returns correct page |
| 1.8 Test retrieval - boundaries | Validation | â¬œ | Query "surcharge threshold" returns exact text |
| 1.9 Measure retrieval precision | Validation | â¬œ | 10 test queries, â‰¥8 return top-3 correct chunks |

**KPI Definition:**
- **Retrieval Accuracy = 90%** means:
  - Given 10 hand-crafted test queries (known answers)
  - At least 9 queries return the correct chunk in top-3 results
  - Test queries stored in `tests/retrieval/known-queries.json`

**Failure Conditions:**
- âŒ Embedding generation fails â†’ STOP (no fallback yet)
- âŒ ChromaDB persistence fails â†’ STOP
- âŒ Retrieval accuracy < 80% â†’ Investigate chunking strategy

---

### Stage 2: Rule Candidates â¬œ NOT STARTED
**Goal:** AI proposes possible rules (never claims truth)

| Task | Status | Success Criteria |
|------|--------|------------------|
| 2.1 Define extraction prompts | â¬œ | Prompts stored in `lib/legal-rag/prompts/*.txt` |
| 2.2 Implement candidate extractor | â¬œ | Returns `{ rule, confidence, citations, ambiguities }` (AI suggests, never approves) |
| 2.3 Handle explicit ambiguity | â¬œ | Returns `{ status: "unclear", reason: "..." }` for conflicts |
| 2.4 Extract tax slabs (test case) | â¬œ | Slabs match PDF exactly, with page refs |
| 2.5 Extract rates & thresholds | â¬œ | Numbers NOT rounded, NOT inferred |
| 2.6 Confidence scoring logic | â¬œ | Low confidence if: citations conflict, proviso missing |
| 2.7 Multi-document cross-check | â¬œ | BLOCKED until Income-tax Act 1961 is ingested (Stage 1 repeated) |

**Ambiguity Representation:**
```json
{
  "status": "unclear",
  "reason": "Multiple surcharge rates found",
  "candidates": [
    {"value": "10%", "source": "page 3, para 2"},
    {"value": "15%", "source": "page 7, Schedule I"}
  ],
  "resolution": null
}
```

**CRITICAL:**
- AI NEVER picks between candidates
- Human or later stage resolves
- Production BLOCKS if unresolved

**KPI:** 
- 0% guessed values = ALL ambiguous fields return `status: "unclear"`
- Track: `% unclear / total fields extracted`

---

### Stage 3: Alignment Validation â¬œ NOT STARTED
**Goal:** Check alignment with source text (NOT correctness)

**Alignment Definition (Explicit):**
- Candidate text matches PDF verbatim (case-insensitive, whitespace-normalized)
- All cited page numbers exist in source PDF
- No proviso/exception omitted within 100 chars of extracted text
- Cross-references (e.g., "as per section X") are intact

**NOT Alignment:**
- Legal correctness (we don't verify law)
- Completeness across entire act
- Interpretation accuracy

| Task | Status | Success Criteria |
|------|--------|------------------|
| 3.1 Define alignment criteria | â¬œ | Written spec (above) |
| 3.2 Text similarity validator | â¬œ | Fuzzy match with threshold 0.95 |
| 3.3 Detect missing provisos | â¬œ | Regex for "Provided that" within context window |
| 3.4 Flag cross-ref failures | â¬œ | Extract section refs, verify they exist in PDF |
| 3.5 Calculate alignment score | â¬œ | Score = (matched_checks / total_checks) |
| 3.6 Generate issues report | â¬œ | JSON list of misalignments |

**KPI:** Average confidence score > 0.8
- Confidence = alignment_score Ã— (1 - ambiguity_count/total_fields)

---

### Stage 4: Schema Mapping â¬œ NOT STARTED
**Goal:** Force rules into predefined structure (AI can't add fields)

| Task | Status | Success Criteria |
|------|--------|------------------|
| Define calculator schema (DSL) | â¬œ | Income tax schema created |
| Create schema validator | â¬œ | Rejects invalid structures |
| Map candidates to schema | â¬œ | All fields typed correctly |
| Handle missing fields | â¬œ | Missing required â†’ explicit fail |
| Type validation | â¬œ | Numbers are numbers, strings are strings |

**KPI:** Schema pass rate

---

### Stage 5: Deterministic Testing â¬œ NOT STARTED
**Goal:** Prove math is stable and safe

| Task | Status | Success Criteria |
|------|--------|------------------|
| Create test framework | â¬œ | Can run tests automatically |
| Boundary tests | â¬œ | Edge cases covered |
| Determinism tests | â¬œ | Same input â†’ same output |
| Regression tests | â¬œ | Previous versions still pass |
| Parity tests (if govt calc exists) | â¬œ | Matches official calculator |
| Generate test reports | â¬œ | Pass/fail clearly shown |

**KPI:** Zero critical test failures

---

### Stage 6: Freeze Gate â¬œ NOT STARTED
**Goal:** Governance - decide if rules can enter production

| Task | Status | Success Criteria |
|------|--------|------------------|
| Define freeze criteria | â¬œ | Clear thresholds set |
| Freeze approval mechanism | â¬œ | Human or automated approval |
| Create frozen rule artifacts | â¬œ | Immutable JSON files |
| Add audit metadata | â¬œ | Who, when, why frozen |
| Version control | â¬œ | Each freeze = new version |

**KPI:** Freeze success rate

---

### Stage 7: Production Execution â¬œ NOT STARTED
**Goal:** Run calculators safely (NO AI, frozen rules only)

| Task | Status | Success Criteria |
|------|--------|------------------|
| Load frozen rules only | â¬œ | Production can't use candidates |
| Deterministic calculator | â¬œ | Pure functions, no randomness |
| Result breakdown | â¬œ | Shows step-by-step calculation |
| Confidence display | â¬œ | Shows rule confidence to user |
| Version logging | â¬œ | Every result logs rule version |
| Error handling | â¬œ | Missing rules â†’ clear error |

**KPI:** Determinism (same input â†’ same output forever)

---

## ğŸ—ï¸ MODULES TO BUILD

### Core Infrastructure (Stage 0-1)
- [x] `legal-rag-system/extract-pdf.mjs` - PDF extraction (pdfjs-dist)
- [x] `legal-rag-system/chunk-text.mjs` - Semantic chunking
- [ ] `lib/legal-rag/embedder.ts` - Embedding generation (OpenAI)
- [ ] `lib/legal-rag/vector-store.ts` - ChromaDB wrapper
- [ ] `lib/legal-rag/retrieval.ts` - Query orchestration

### Retrieval Layer (Stage 1)
- [ ] `lib/legal-rag/search/semantic-search.ts` - Vector similarity search
- [ ] `lib/legal-rag/search/reranker.ts` - (Optional) Re-rank by page proximity
- [ ] `tests/retrieval/known-queries.json` - Test query bank
- [ ] `tests/retrieval/test-search.ts` - Automated retrieval tests

### Rule Extraction (Stage 2-3)
- [ ] `lib/legal-rag/prompts/extract-tax-slabs.txt` - Slab extraction prompt
- [ ] `lib/legal-rag/prompts/extract-thresholds.txt` - Threshold extraction
- [ ] `lib/legal-rag/extraction/candidate-generator.ts` - LLM wrapper (generates candidates, NEVER validates)
- [ ] `lib/legal-rag/extraction/ambiguity-detector.ts` - Conflict detector
- [ ] `lib/legal-rag/validation/alignment-checker.ts` - Text alignment
- [ ] `lib/legal-rag/validation/cross-ref-validator.ts` - Section reference checker

### Schema & Control (Stage 4)
- [ ] `lib/legal-rag/schemas/income-tax-2024.ts` - FY 2024-25 schema (Zod)
- [ ] `lib/legal-rag/schemas/validator.ts` - Runtime schema validator

### Testing (Stage 5)
- [ ] `tests/boundary/tax-slabs.test.ts` - Slab boundary tests
- [ ] `tests/determinism/same-input.test.ts` - Determinism tests
- [ ] `tests/regression/fy-2023-24-parity.test.ts` - Previous year comparison

### Governance (Stage 6)
- [ ] `lib/legal-rag/governance/freeze.ts` - Freeze mechanism
- [ ] `rules_final/income_tax/FY_2024_25/frozen.json` - Frozen rules artifact

### Production (Stage 7)
- [ ] `lib/legal-rag/production/calculator.ts` - Deterministic executor
- [ ] `app/api/legal-rag/calculate/route.ts` - Production API

---

## ğŸš¦ STOP CONDITIONS

**STOP IMMEDIATELY IF:**
- âŒ AI invents values (no null checks)
- âŒ Rules auto-promote to production
- âŒ Tests are skipped
- âŒ Confidence scores ignored
- âŒ PDFs are modified
- âŒ Frozen rules change
- âŒ LLM called in production API endpoint (Stage 7)
- âŒ Confidence threshold bypassed

**Stopping early = success, not failure**

---

## ğŸ“ˆ SUCCESS METRICS

### Per Stage
- Stage 0: Hash stability âœ…
- Stage 1: Retrieval accuracy â‰¥ 90% (9/10 known queries return correct chunk in top-3)
- Stage 2: 0% guessed values
- Stage 3: Average confidence > 0.8
- Stage 4: Schema pass rate > 95%
- Stage 5: Zero critical failures
- Stage 6: Clear approval audit trail
- Stage 7: Determinism verified (same input â†’ same output, 10k test runs)

### Overall System
- 97-99% reliability on test cases
- Full traceability (result â†’ rule â†’ PDF â†’ hash)
- Zero production AI usage
- Clear confidence scores on all outputs

---

## ğŸ“ CURRENT STATUS

**Stage 0: COMPLETE âœ…**
- Finance Act 2024 stored
- Hash: `61c6ab8909b8fffc735973c0e0188631b4eb3d1d0618c321bdffb0e91737c19b`
- 26 pages, official source
- Commits: 31de45d (extraction), 1c344e4 (chunking)
- Next commit: PROJECT_PLAN audit corrections

**Stage 1: 60% COMPLETE âš™ï¸**
- âœ… 1.1-1.3: PDF parsing, chunking (semantic-preserving)
- â¬œ 1.4-1.9: Vector DB, embeddings, search testing

**Next Immediate Steps:**
1. Install ChromaDB (`npm install chromadb`)
2. Generate embeddings for 44 chunks
3. Test semantic search with 10 known queries
4. Measure retrieval accuracy before moving to Stage 2

---

## ğŸ“… TIMELINE APPROACH

**Not time-based, gate-based:**
- Each stage must pass before next begins
- No rushing to "get something working"
- Quality gates are mandatory
- Better to stop at Stage 3 with confidence than rush to Stage 7 with bugs

---

## ğŸ¯ IMMEDIATE NEXT STEPS

### Completed:
- âœ… **Stage 1.1:** PDF parsing (pdfjs-dist)
- âœ… **Stage 1.2:** Text extraction (26 pages)
- âœ… **Stage 1.3:** Semantic chunking (44 chunks)
- âœ… **Stage 1.3.1:** Manual chunk inspection (verified no mid-sentence breaks)

### In Progress:
- âš™ï¸ **Stage 1.4:** Install ChromaDB
- â¬œ **Stage 1.5:** Generate embeddings (OpenAI)
- â¬œ **Stage 1.6:** Store in vector DB
- â¬œ **Stage 1.7-1.9:** Test retrieval accuracy

**Each substep will be tested before proceeding.**
